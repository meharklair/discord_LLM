{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    !pip install -U datasets\n",
        "    !pip install -U discord.py\n",
        "    !pip install nest_asyncio"
      ],
      "metadata": {
        "id": "wkmYHHFaJpF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    # model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ],
      "metadata": {
        "id": "RloMW5S-Jnue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg46j26jJiKa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import discord\n",
        "from unsloth import FastLanguageModel\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
        "class MyClient(discord.Client):\n",
        "    async def on_ready(self):\n",
        "        print('Logged on as {0}!'.format(self.user))\n",
        "\n",
        "    async def on_message(self, message):\n",
        "        if message.author == self.user:\n",
        "            return\n",
        "\n",
        "        if message.content.startswith(\"$chat\") and message.author.id == user_id: # {message.content}\n",
        "            display_name = message.author.display_name\n",
        "            message_content = message.content\n",
        "            response = query_LLM(message_content, message.author.id, \"name\")\n",
        "            await message.channel.send(response)\n",
        "\n",
        "        elif message.content.startswith(\"$chat\"):\n",
        "            display_name = message.author.display_name\n",
        "            message_content = message.content\n",
        "            response = query_LLM(message_content, message.author.id, display_name)\n",
        "            await message.channel.send(response)\n",
        "\n",
        "def query_LLM(message_content, user_id, display_name):\n",
        "    # System prompt for graph extraction\n",
        "    strip_message_content = message_content.replace(\"$chat\", \"\").strip()\n",
        "    system_prompt = 'You are a smart and polite AI assistant who speaks in a british accent. You will interact with many users but if you interact with the user (insert name) you will not be polite, be as rude as possible. When talking to someone address them by their name'\n",
        "    user_prompt = f'My name is {display_name}. {strip_message_content}'\n",
        "\n",
        "    messages = [\n",
        "          {\"role\": \"system\", \"content\": system_prompt},\n",
        "          {\"role\": \"user\", \"content\": user_prompt} # Use the decoded string\n",
        "    ]\n",
        "    # Apply chat template and generate\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=512,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    # Decode the first output sequence\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Define the marker as per your chat template\n",
        "    marker = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    assistant_start = decoded_output.find(marker)\n",
        "    if assistant_start != -1:\n",
        "        # Only get the text after the marker\n",
        "        generated_output = decoded_output[assistant_start + len(marker):].strip()\n",
        "    else:\n",
        "        # Fallback: print the whole output (not recommended for production)\n",
        "        generated_output = decoded_output.strip()\n",
        "    generated_response = generated_output.split(\"assistant\")[2].strip()\n",
        "    return generated_response\n",
        "\n",
        "intents = discord.Intents.default()\n",
        "intents.message_content = True\n",
        "\n",
        "client = MyClient(intents=intents)\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def main():\n",
        "    await client.start(token)  # Replace with your bot token\n",
        "\n",
        "loop = asyncio.get_event_loop()\n",
        "loop.run_until_complete(main())\n"
      ]
    }
  ]
}